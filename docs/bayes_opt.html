---

title: Bayes_opt decorator


keywords: fastai
sidebar: home_sidebar



nb_path: "01_bayes_opt.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_bayes_opt.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a decorator that automatically deals with running the bayes-opt package and logging and reloading the progress related to an estimator/model. 
This is a function I adopted from <a href="https://gist.github.com/VincentGatien/882b3bbd81ff98b426ac418c45cfc1bd">https://gist.github.com/VincentGatien/882b3bbd81ff98b426ac418c45cfc1bd</a>  Thanks to the original author! 
I made minimal changes to tailor to my use cases.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">BernoulliNB</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_data</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Synthetic binary classification dataset.&quot;&quot;&quot;</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span>
        <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">n_features</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span>
        <span class="n">n_informative</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">n_redundant</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">134985745</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">targets</span>


<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="optimize_bayes_param" class="doc_header"><code>optimize_bayes_param</code><a href="https://github.com/DavidykZhao/Yikai_helper_funcs/tree/master/Yikai_helper_funcs/bayes_opt.py#L18" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>optimize_bayes_param</code>(<strong><code>X</code></strong>, <strong><code>y</code></strong>, <strong>*<code>args_objective</code></strong>, <strong><code>objective_fn</code></strong>=<em><code>cross_val_score</code></em>, <strong>**<code>kwargs_objective</code></strong>)</p>
</blockquote>
<p>The first closure passes in X, y and the objective funtion, which defaults to the cross_val_score function from sklearn.</p>
<p>:param objective_fn:
    objective function to optimize
:param X: np.array
    Matrix of features
:param y: np.array
    Vectors of labels
:param args_eval:
    *args passed to objective_fn
:param kwargs_eval:
    **kwargs passed to objective_fn</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pbounds_forest</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]),</span>
    <span class="s2">&quot;min_samples_split&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
<span class="p">}</span>
<span class="c1"># Notice that the parameters in pbounds should match with those in the optimiz_func. </span>

<span class="nd">@optimize_bayes_param</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">optimize_forest</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RandomForestClassifier</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">),</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">min_samples_split</span><span class="p">),</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that once we do not change the default argument <code>fit = True</code>, the returned object is the best fitted model itself; it <code>fit = False</code>, then nothing is returned and the logger tracked the loss from the corresponding hyperparameters</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_rf</span> <span class="o">=</span> <span class="n">optimize_forest</span><span class="p">(</span><span class="n">init_points</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">pbounds</span><span class="o">=</span><span class="n">pbounds_forest</span><span class="p">,</span> <span class="n">log_dir</span><span class="o">=</span><span class="s2">&quot;./bayes_opt_logs/forest&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The best combination of hyperparameters are {&#39;min_samples_split&#39;: 14.590034541800147, &#39;n_estimators&#39;: 287.81893667937186}
The best score for the hyperparameters are 0.877
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can see that the best model returned indeed carried over the best hyperparameters.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_rf</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;bootstrap&#39;: True,
 &#39;ccp_alpha&#39;: 0.0,
 &#39;class_weight&#39;: None,
 &#39;criterion&#39;: &#39;gini&#39;,
 &#39;max_depth&#39;: None,
 &#39;max_features&#39;: &#39;auto&#39;,
 &#39;max_leaf_nodes&#39;: None,
 &#39;max_samples&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;n_estimators&#39;: 464,
 &#39;n_jobs&#39;: -1,
 &#39;oob_score&#39;: False,
 &#39;random_state&#39;: None,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Check if we can pass np.array to pbounds</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pbounds_forest</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]),</span>
    <span class="s2">&quot;min_samples_split&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
<span class="p">}</span>
<span class="c1"># Notice that the parameters in pbounds should match with those in the optimiz_func. </span>

<span class="nd">@optimize_bayes_param</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">optimize_forest</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RandomForestClassifier</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">),</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">min_samples_split</span><span class="p">),</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_rf</span> <span class="o">=</span> <span class="n">optimize_forest</span><span class="p">(</span><span class="n">init_points</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">pbounds</span><span class="o">=</span><span class="n">pbounds_forest</span><span class="p">,</span> <span class="n">log_dir</span><span class="o">=</span><span class="s2">&quot;./bayes_opt_logs/forest&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The best combination of hyperparameters are {&#39;min_samples_split&#39;: 2.947705047766325, &#39;n_estimators&#39;: 464.7807713393563}
The best score for the hyperparameters are 0.883
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

