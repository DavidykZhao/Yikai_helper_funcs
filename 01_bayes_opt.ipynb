{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp tab_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This is a decorator that automatically deals with running the bayes-opt package and logging and reloading the progress related to an estimator/model. \n",
    "This is a function I adopted from https://gist.github.com/VincentGatien/882b3bbd81ff98b426ac418c45cfc1bd  Thanks to the original author! \n",
    "I made minimal changes to tailor to my use cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "from pathlib import Path\n",
    "from functools import wraps\n",
    "from typing import Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data():\n",
    "    \"\"\"Synthetic binary classification dataset.\"\"\"\n",
    "    data, targets = make_classification(\n",
    "        n_samples=1000,\n",
    "        n_features=45,\n",
    "        n_informative=12,\n",
    "        n_redundant=7,\n",
    "        random_state=134985745,\n",
    "    )\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "x, y = get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def optimize_bayes_param(X, y, objective_fn=cross_val_score, *args_objective, **kwargs_objective):\n",
    "    \"\"\"\n",
    "    The first closure passes in X, y and the objective funtion, which defaults to the cross_val_score function from sklearn. \n",
    "\n",
    "\n",
    "    :param objective_fn:\n",
    "        objective function to optimize\n",
    "    :param X: np.array\n",
    "        Matrix of features\n",
    "    :param y: np.array\n",
    "        Vectors of labels\n",
    "    :param args_eval:\n",
    "        *args passed to objective_fn\n",
    "    :param kwargs_eval:\n",
    "        **kwargs passed to objective_fn\n",
    "    \"\"\"\n",
    "\n",
    "    def optimize_bayes_wo_param(parse_model_params):\n",
    "\n",
    "        def _opt_engine(*args_model, **kwargs_model):\n",
    "            \"\"\"\n",
    "            This is the running engine function that takes an estimator/model object and a loss function \n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            estimator = parse_model_params(*args_model, **kwargs_model)\n",
    "            return objective_fn(estimator, X=X, y=y, *args_objective, **kwargs_objective).mean()\n",
    "\n",
    "        @wraps(parse_model_params)\n",
    "        def run_trials(pbounds: Dict,\n",
    "                       init_points: int = 10,\n",
    "                       n_iter: int = 10,\n",
    "                       log_dir: Path = Path(\"./bayes_opt_logs\"),\n",
    "                       acq: str = 'ucb',\n",
    "                       kappa: str = 2.576,\n",
    "                       fit: bool = True):\n",
    "            \"\"\"\n",
    "            :param pbounds: dict\n",
    "                Dictionary with parameters names as keys and a tuple with minimum\n",
    "                and maximum values.\n",
    "            :param init_points : int\n",
    "                Number of iterations before the explorations starts the exploration\n",
    "                for the maximum.\n",
    "            :param n_iter: int\n",
    "                Number of iterations where the method attempts to find the maximum\n",
    "                value.\n",
    "            :param log_dir: Path\n",
    "                Directory to log json results\n",
    "            :param acq: str\n",
    "                The acquisition method used.\n",
    "            :param kappa: float\n",
    "                Parameter to indicate how closed are the next parameters sampled.\n",
    "                Higher value = favors spaces that are least explored.\n",
    "                Lower value = favors spaces where the regression function is the\n",
    "                highest.\n",
    "            :param fit: bool\n",
    "                if True the best model is fitted on de data\n",
    "                if False the best model is returned unfitted\n",
    "            :return: A Sklearn model with the pbounds hyperparameters optimized\n",
    "             by Bayesian Optimisation in cross-validation\n",
    "            \"\"\"\n",
    "            optimizer = BayesianOptimization(_opt_engine, pbounds=pbounds)\n",
    "            log_dir = Path(log_dir)\n",
    "            if log_dir.exists():\n",
    "                all_log = [str(path) for path in log_dir.iterdir()]\n",
    "                load_logs(optimizer, logs=all_log)\n",
    "                filename = 'log_{}.json'.format(len(all_log))\n",
    "            else:\n",
    "                log_dir.mkdir(parents=True)\n",
    "                filename = 'log_0.json'\n",
    "            logger = JSONLogger(path=str(log_dir / filename))\n",
    "            optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "            optimizer.maximize(init_points, n_iter, kappa=kappa, acq=acq)\n",
    "            print(f\"The best combination of hyperparameters are { optimizer.max['params'] }\")\n",
    "            best_model = parse_model_params(**optimizer.max['params'])\n",
    "            if fit:\n",
    "                best_model.fit(X=X, y=y)\n",
    "            return best_model\n",
    "\n",
    "        return run_trials\n",
    "\n",
    "    return optimize_bayes_wo_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds_forest = {\n",
    "    'n_estimators': (10, 1000),\n",
    "    \"min_samples_split\": (2, 50),\n",
    "}\n",
    "# Notice that the parameters in pbounds should match with those in the optimiz_func. \n",
    "\n",
    "@optimize_bayes_param(X=x, y=y)\n",
    "def optimize_forest(n_estimators: float, min_samples_split: float) -> RandomForestClassifier:\n",
    "    return RandomForestClassifier(n_estimators=int(n_estimators), min_samples_split=int(min_samples_split),\n",
    "                                      n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that once we do not change the default argument `fit = True`, the returned object is the best fitted model itself; it `fit = False`, then nothing is returned and the logger tracked the loss from the corresponding hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best combination of hyperparameters are {'min_samples_split': 9.286943598933544, 'n_estimators': 338.51384056305545}\n"
     ]
    }
   ],
   "source": [
    "best_rf = optimize_forest(init_points=5, n_iter=10, pbounds=pbounds_forest, log_dir=Path(\"./bayes_opt_logs/forest\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the best model returned indeed carried over the best hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 9,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 338,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dc31fd7c26e2425bbd851b0f8cbbcd5639dbbd39b944c75542f3e50c42eb2a32"
    }
   },
   "name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
